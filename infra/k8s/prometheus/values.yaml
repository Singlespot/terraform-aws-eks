alertmanagerFiles:
  alertmanager.yml:
    global:
      resolve_timeout: 1m
      slack_api_url: 'https://hooks.slack.com/services/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'

    receivers:
      - name: slack
        slack_configs:
          - channel: '#pulsar-alerts-dev'
            icon_url: https://avatars3.githubusercontent.com/u/3380462
            send_resolved: true
            title: '{{ template "custom_title" . }}'
            text: '{{ template "custom_slack_message" . }}'

    route:
#      group_wait: 10s
#      group_interval: 5m
      receiver: slack
#      repeat_interval: 3h

    # https://medium.com/quiq-blog/better-slack-alerts-from-prometheus-49125c8c672b
    # https://harthoover.com/pretty-alertmanager-alerts-in-slack/
    # https://github.com/helm/charts/pull/8230
    templates:
      - /etc/config/notifications*.tmpl

  notificationsSlack.tmpl: |
    {{ define "__single_message_title" }}{{ range .Alerts.Firing }}{{ .Labels.alertname }} @ {{ .Annotations.summary }}{{ end }}{{ range .Alerts.Resolved }}{{ .Labels.alertname }} @ {{ .Annotations.summary }}{{ end }}{{ end }}{{ define "custom_title" }}[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }}{{ template "__single_message_title" . }}{{ end }}{{ end }}{{ define "custom_slack_message" }}
    {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }}
    {{ range .Alerts.Firing }}{{ .Annotations.description }}{{ end }}{{ range .Alerts.Resolved }}{{ .Annotations.description }}{{ end }}
    {{ else }}
    {{ if gt (len .Alerts.Firing) 0 }}
    *Alerts Firing:*
    {{ range .Alerts.Firing }}- {{ .Annotations.summary }}: {{ .Annotations.description }}
    {{ end }}{{ end }}
    {{ if gt (len .Alerts.Resolved) 0 }}
    *Alerts Resolved:*
    {{ range .Alerts.Resolved }}- {{ .Annotations.summary }}: {{ .Annotations.description }}
    {{ end }}{{ end }}
    {{ end }}
    {{ end }}


serverFiles:
  ## Alerts configuration
  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
  alerting_rules.yml:
    groups:
      - name: Mixed
        rules:
          - alert: InstanceDown
            expr: up == 0
            for: 1m
            labels:
              severity: 'critical'
            annotations:
              title: 'Instance {{ $labels.instance }} down'
              summary: 'Instance {{ $labels.instance }} down'
              description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 1 minute.\n  LABELS: {{ $labels }}"

      - name: Nodes
        rules:
          - alert: HostOutOfDiskSpace
            expr: (node_filesystem_avail_bytes{mountpoint="/rootfs"}  * 100) / node_filesystem_size_bytes{mountpoint="/rootfs"} < 10
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Host out of disk space (instance {{ $labels.instance }})"
              description: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: HostHighCpuLoad
            expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Host high CPU load (instance {{ $labels.instance }})"
              description: "CPU load is > 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - name: Containers
        rules:
          - alert: ContainerKilled
            expr: time() - container_last_seen > 60
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Container killed (instance {{ $labels.instance }})"
              description: "A container has disappeared\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ContainerCpuUsage
            expr: (sum(rate(container_cpu_usage_seconds_total{name=~".+"}[3m])) BY (instance, name) * 100) > 80
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Container CPU usage (instance {{ $labels.instance }})"
              description: "Container CPU usage is above 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ContainerMemoryUsage
            expr: (sum(container_memory_usage_bytes) BY (instance, pod_name, container_name) / sum(container_spec_memory_limit_bytes{pod_name=~".+",container_name=~".+"} > 0) BY (instance, pod_name, container_name) * 100) > 80
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Container memory usage is high ({{ $labels.pod_name }})"
              description: "Container memory usage is above 80%\n  VALUE = {{ $value }}\n  Pod name: {{ $labels.pod_name }}\n  Container name: {{ $labels.container_name }}\n  Instance: {{ $labels.instance }}"
          - alert: ContainerVolumeUsage
            expr: (1 - (sum(container_fs_inodes_free) BY (instance) / sum(container_fs_inodes_total) BY (instance)) * 100) > 80
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Container Volume usage (instance {{ $labels.instance }})"
              description: "Container Volume usage is above 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - name: ElasticSearch
        rules:
          - alert: ElasticsearchHeapUsageTooHigh
            expr: (elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"}) * 100 > 90
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Elasticsearch Heap Usage Too High (instance {{ $labels.instance }})"
              description: "The heap usage is over 90% for 5m\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchHeapUsageWarning
            expr: (elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"}) * 100 > 80
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Elasticsearch Heap Usage warning (instance {{ $labels.instance }})"
              description: "The heap usage is over 80% for 5m\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchDiskSpaceLow
            expr: elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes * 100 < 20
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Elasticsearch disk space low (instance {{ $labels.instance }})"
              description: "The disk usage is over 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchDiskOutOfSpace
            expr: elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes * 100 < 10
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Elasticsearch disk out of space (instance {{ $labels.instance }})"
              description: "The disk usage is over 90%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchClusterRed
            expr: elasticsearch_cluster_health_status{color="red"} == 1
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Elasticsearch Cluster Red (instance {{ $labels.instance }})"
              description: "Elastic Cluster Red status\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchClusterYellow
            expr: elasticsearch_cluster_health_status{color="yellow"} == 1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Elasticsearch Cluster Yellow (instance {{ $labels.instance }})"
              description: "Elastic Cluster Yellow status\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchHealthyNodes
            expr: elasticsearch_cluster_health_number_of_nodes < number_of_nodes
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Elasticsearch Healthy Nodes (instance {{ $labels.instance }})"
              description: "Number Healthy Nodes less then number_of_nodes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchHealthyDataNodes
            expr: elasticsearch_cluster_health_number_of_data_nodes < number_of_data_nodes
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Elasticsearch Healthy Data Nodes (instance {{ $labels.instance }})"
              description: "Number Healthy Data Nodes less then number_of_data_nodes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchRelocationShards
            expr: elasticsearch_cluster_health_relocating_shards > 0
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Elasticsearch relocation shards (instance {{ $labels.instance }})"
              description: "Number of relocation shards for 20 min\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchInitializingShards
            expr: elasticsearch_cluster_health_initializing_shards > 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Elasticsearch initializing shards (instance {{ $labels.instance }})"
              description: "Number of initializing shards for 10 min\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchUnassignedShards
            expr: elasticsearch_cluster_health_unassigned_shards > 0
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Elasticsearch unassigned shards (instance {{ $labels.instance }})"
              description: "Number of unassigned shards for 2 min\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchPendingTasks
            expr: elasticsearch_cluster_health_number_of_pending_tasks > 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Elasticsearch pending tasks (instance {{ $labels.instance }})"
              description: "Number of pending tasks for 10 min. Cluster works slowly.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchNoNewDocuments
            expr: rate(elasticsearch_indices_docs{es_data_node="true"}[10m]) < 1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Elasticsearch no new documents (instance {{ $labels.instance }})"
              description: "No new documents for 10 min!\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - name: Kubernetes
        rules:
          - alert: KubernetesNodeReady
            expr: kube_node_status_condition{condition="Ready",status="true"} == 0
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes Node ready (instance {{ $labels.instance }})"
              description: "Node {{ $labels.node }} has been unready for a long time\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesMemoryPressure
            expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes memory pressure (instance {{ $labels.instance }})"
              description: "{{ $labels.node }} has MemoryPressure condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesDiskPressure
            expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes disk pressure (instance {{ $labels.instance }})"
              description: "{{ $labels.node }} has DiskPressure condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesOutOfDisk
            expr: kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes out of disk (instance {{ $labels.instance }})"
              description: "{{ $labels.node }} has OutOfDisk condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesJobFailed
            expr: kube_job_status_failed > 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes Job failed (instance {{ $labels.instance }})"
              description: "Job {{$labels.namespace}}/{{$labels.exported_job}} failed to complete\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesCronjobSuspended
            expr: kube_cronjob_spec_suspend != 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes CronJob suspended (instance {{ $labels.instance }})"
              description: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is suspended\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesPersistentvolumeclaimPending
            expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes PersistentVolumeClaim pending (instance {{ $labels.instance }})"
              description: "PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesVolumeOutOfDiskSpace
            expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes Volume out of disk space (instance {{ $labels.instance }})"
              description: "Volume is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesVolumeFullInFourDays
            expr: predict_linear(kubelet_volume_stats_available_bytes[6h], 4 * 24 * 3600) < 0
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes Volume full in four days (instance {{ $labels.instance }})"
              description: "{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ $value | humanize }}% is available.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesPersistentvolumeError
            expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"} > 0
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes PersistentVolume error (instance {{ $labels.instance }})"
              description: "Persistent volume is in bad state\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesStatefulsetDown
            expr: (kube_statefulset_status_replicas_ready / kube_statefulset_status_replicas_current) != 1
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes StatefulSet down (instance {{ $labels.instance }})"
              description: "A StatefulSet went down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesHpaScalingAbility
            expr: kube_hpa_status_condition{condition="false", status="AbleToScale"} == 1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes HPA scaling ability (instance {{ $labels.instance }})"
              description: "Pod is unable to scale\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesHpaMetricAvailability
            expr: kube_hpa_status_condition{condition="false", status="ScalingActive"} == 1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes HPA metric availability (instance {{ $labels.instance }})"
              description: "HPA is not able to colelct metrics\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesHpaScaleCapability
            expr: kube_hpa_status_desired_replicas >= kube_hpa_spec_max_replicas
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes HPA scale capability (instance {{ $labels.instance }})"
              description: "The maximum number of desired Pods has been hit\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesPodNotHealthy
            expr: min_over_time(sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"} == 1)[1h:])
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes Pod not healthy (instance {{ $labels.instance }})"
              description: "Pod has been in a non-ready state for longer than an hour.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesPodCrashLooping
            expr: rate(kube_pod_container_status_restarts_total[15m]) * 60 * 5 > 5
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes pod crash looping (instance {{ $labels.instance }})"
              description: "Pod {{ $labels.pod }} is crash looping\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesReplicassetMismatch
            expr: kube_replicaset_spec_replicas != kube_replicaset_status_ready_replicas
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes ReplicasSet mismatch (instance {{ $labels.instance }})"
              description: "Deployment Replicas mismatch\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesDeploymentReplicasMismatch
            expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes Deployment replicas mismatch (instance {{ $labels.instance }})"
              description: "Deployment Replicas mismatch\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesStatefulsetReplicasMismatch
            expr: kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes StatefulSet replicas mismatch (instance {{ $labels.instance }})"
              description: "A StatefulSet has not matched the expected number of replicas for longer than 15 minutes.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesDeploymentGenerationMismatch
            expr: kube_deployment_status_observed_generation != kube_deployment_metadata_generation
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes Deployment generation mismatch (instance {{ $labels.instance }})"
              description: "A Deployment has failed but has not been rolled back.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesStatefulsetGenerationMismatch
            expr: kube_statefulset_status_observed_generation != kube_statefulset_metadata_generation
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes StatefulSet generation mismatch (instance {{ $labels.instance }})"
              description: "A StatefulSet has failed but has not been rolled back.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesStatefulsetUpdateNotRolledOut
            expr: max without (revision) (kube_statefulset_status_current_revision unless kube_statefulset_status_update_revision) * (kube_statefulset_replicas != kube_statefulset_status_replicas_updated)
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes StatefulSet update not rolled out (instance {{ $labels.instance }})"
          - alert: KubernetesDaemonsetRolloutStuck
            expr: kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 < 100 or kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes DaemonSet rollout stuck (instance {{ $labels.instance }})"
              description: "Some Pods of DaemonSet are not scheduled or not ready\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesDaemonsetMisscheduled
            expr: kube_daemonset_status_number_misscheduled > 0
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes DaemonSet misscheduled (instance {{ $labels.instance }})"
              description: "Some DaemonSet Pods are running where they are not supposed to run\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesCronjobTooLong
            expr: time() - kube_cronjob_next_schedule_time > 3600
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes CronJob too long (instance {{ $labels.instance }})"
              description: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more than 1h to complete.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesJobCompletion
            expr: kube_job_spec_completions - kube_job_status_succeeded > 0 or kube_job_status_failed > 0
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes job completion (instance {{ $labels.instance }})"
              description: "Kubernetes Job failed to complete\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesApiServerErrors
            expr: sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[2m])) / sum(rate(apiserver_request_count{job="apiserver"}[2m])) * 100 > 3
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes API server errors (instance {{ $labels.instance }})"
              description: "Kubernetes API server is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesApiClientErrors
            expr: (sum(rate(rest_client_requests_total{code=~"(4|5).."}[2m])) by (instance, job) / sum(rate(rest_client_requests_total[2m])) by (instance, job)) * 100 > 1
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes API client errors (instance {{ $labels.instance }})"
              description: "Kubernetes API client is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesClientCertificateExpiresNextWeek
            expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 7*24*60*60
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes client certificate expires next week (instance {{ $labels.instance }})"
              description: "A client certificate used to authenticate to the apiserver is expiring next week.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesClientCertificateExpiresSoon
            expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 24*60*60
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes client certificate expires soon (instance {{ $labels.instance }})"
              description: "A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesApiServerLatency
            expr: histogram_quantile(0.99, sum(apiserver_request_latencies_bucket{verb!~"CONNECT|WATCHLIST|WATCH|PROXY"}) WITHOUT (instance, resource)) / 1e+06 > 1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes API server latency (instance {{ $labels.instance }})"
              description: "Kubernetes API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: CorednsPanicCount
            expr: increase(coredns_panic_count_total[10m]) > 0
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "CoreDNS Panic Count (instance {{ $labels.instance }})"
              description: "Number of CoreDNS panics encountered\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
